[
{
	"uri": "/",
	"title": "AWS DataSync",
	"tags": [],
	"description": "",
	"content": "Migration Data to AWS using DataSync Overall In this lab you will simulate the migration of a legacy on-premises file system to S3. You will use DataSync to migrate data from on-premises to AWS. You will then use Storage Gateway to present a file system back to on-premises.\nContent Content  Introduction  Preparation Data Migration Clean Up Resources  "
},
{
	"uri": "/2-prerequiste/2.1-create-s3/",
	"title": "Create S3 ",
	"tags": [],
	"description": "",
	"content": " Go to S3 service management console, preferrably in the us-west-2 region   Click Create bucket In the Bucket name, enter migration-data-to-aws-lab Leave everything else default. Click Create bucket  In the S3 bucket service management, click S3 bucket just created.   Click Create folder Enter Folder name: assets Click Create folder  Download the CloudFormation templates blow   Migrating Data to AWS (On-premise) Migrating Data to AWS (Cloud)  Upload all the files you just downloaded to the assets foldet of S3 bucket.  "
},
{
	"uri": "/1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "AWS DataSync is a managed data transfer service offered by Amazon Web Services (AWS). It enables you to easily and securely synchronize data between on-premises storage systems and various AWS storage services, including:\n Amazon S3 Amazon EFS (Elastic File System) Amazon FSx for Windows File Server Amazon FSx for Lustre.  Use Cases for AWS DataSync:\n On-premises Backup to S3: Regularly back up your on-premises data to S3 for disaster recovery and long-term archiving. Cloud Migration: Migrate your on-premises file shares or databases to AWS storage services for scalability and easier management. Data Sharing: Synchronize data between different locations for collaboration and data analysis purposes. Continuous Replication: Keep your on-premises data and its corresponding cloud copy in sync for real-time data availability.  How AWS DataSync Works:\n Configure a DataSync Task: Define the source and destination of your data transfer, including specific folders or buckets. Schedule the Task: Choose when the data transfer should occur (e.g., hourly, daily, or on a specific date). Run the Task: DataSync initiates the transfer according to your schedule, encrypting data during transfer and at rest. Monitor and Manage: Track the progress of your data transfers and manage tasks through the AWS Management Console or AWS CLI.  "
},
{
	"uri": "/3-data-migration/3.1-copy-to-s3/",
	"title": "Onpremises file copy to S3 using DataSync",
	"tags": [],
	"description": "",
	"content": "Activate the DataSync agent Although the agent instance was created by CloudFormation, before it can be used it needs to be activated in the in-cloud region. Ensure you are in the us-east-1 (N.Virginia) region. Follow the steps below to active the agent.\nFrom the AWS console, click Services and select DataSync\n Click the Transfer data button. On the left hand column click on Agents then click on Create agent button.  In tab Create agent\n Leave the Deploy agent by default. The agent has already been deployed to an instance in the on-premises (us-west-2) region. The Service endpoint should be set to Public service endpoints in US East (N. Virgina). Under the Activation key section, enter the Public IP address of the DataSync agent instance running in the on-premises region. Click Get key.  Leave defaults and click Create agent. Copy the agent-id to your favorite text editor.\nCreate an NFS Location On the left-hand side of the DataSync service page, click Locations \u0026gt; Create location.\n From the Location type drop-down, select Network File System (NFS). From the Agents drop-down select the agent we just created. In the NFS Server box, enter the private IP address of the NFS Server. Under Mount path enter /media/data. Click Create location.  Create an S3 Location On the left-hand side of the DataSync service page, click Locations.\nClick Create location.\n From the Location type drop-down select Amazon S3 Bucket. From the S3 bucket drop-down select the sid-datasync-xxxxxxx bucket. Leave S3 storage class set to Standard. Under Folder, enter /. This will copy all files to the top-level bucket.  Click Create location.\nClick Locations on the left side of the page. You should have two locations, one for the NFS server and one for the S3 bucket.\nCreate a task On the left-hand side of the DataSync service page, click on Tasks \u0026gt; Create task\nIn Step 1 - Configure source location\n Under Source location options select Choose an existing location. Under Choose existing location select the NFS server location we created earlier. Your screen should look similar to the following. Click Next.  In Step 2 - Configure destination location\n Under Destination location options select Choose an existing location. Under Choose existing location select the S3 location we created earlier. Your screen should look similar to the following. Click Next.  On the bottom of the Configure settings page click Autogenerate to autogenerate a CloudWatch log group. Leave everything else default. Click Next.\nOn the Review page leave all the defaults and click Create task.\n title : \u0026ldquo;Onpremises file copy to S3 using DataSync\u0026rdquo; date : \u0026ldquo;r Sys.Date()\u0026rdquo; weight : 1 chapter : false pre : \u0026quot; 3.1. \u0026quot; Run the task Ensure the Task status is available. It may take a few minutes.\nClick Start, leave the default settings, and click Start. The Task status will change to running.\nAs the task runs, the execution status will progress from \u0026ldquo;Launching\u0026rdquo; to \u0026ldquo;Preparing\u0026rdquo; to \u0026ldquo;Transferring\u0026rdquo; to \u0026ldquo;Verifying\u0026rdquo; and finally to \u0026ldquo;Success\u0026rdquo;.\nThe transfer will take a few minutes. Transferring may be at 0% and then finish immediately. This is because once the task starts, there is only a small amount of data transferred.\n\rOnce complete the Execution status changes to success. You can see that we transferred 202 files. This is for 200 files in the data set and the two folders in the data path.\nValidate the data transfer Go to S3 service management console\nSelect the sid-datasync-xxxxxxx bucket.\nClick the images folder. You will see the 200 objects you migrated from the on-premises NFS server.\nIn this module you successfully activated the DataSync agents and created a task to copy files from the on-premises NFS server into the S3 bucket. You then verified that the files were copied successfully.\n"
},
{
	"uri": "/3-data-migration/3.2-file-gateway/",
	"title": "Acces S3 bucket from on-premises using File Gateway",
	"tags": [],
	"description": "",
	"content": "You now have the files from the NFS server copied to your S3 bucket. In this module, you will configure the File Gateway in the on-premises region to connect to your S3 bucket and provide access to the files in the bucket through an NFS share. You will mount the File Gateway share on the Application server to validate access to the files.\n1. Activate the File Gateway Just as you activated the DataSync agent in the previous module, you need to perform a similar step for the File Gateway, activating it in the in-cloud (us-east-1) region. Follow the steps below to activate the gateway.\nFrom the AWS console, click Services and select StorageGateway.\nClick the Create gateway button.\n Give the Gateway a name DataMigrationGateway Select Amazon S3 File Gateway and scroll to the next section Platform options.   Select Amazon EC2 as the host platform. The gateway was already deployed for you so check the box at the bottom. Select Customize your settings. Do not click on Launch instance At the bottom of the page check the box to Confirm set up gateway is completed  Click Next\n2. Connect to AWS Select IP adress, Enter the Public IP address of the File Gateway. You should have copied this IP from the CloudFormation Outputs tab to your favorite text editor. Click Next.\n3. Review and activate Review the details on the screen, should look similar to this, if so Click Activate gateway\n4. Configure cache Storage Wait for a few seconds while the Disk ID loads, otherwise you will have to assign them manually.\n Once it loads you should see Disk ID /dev/sdc. Ensure Allocated to is set to Cache. Leave all settings to the Default values and Click Configure  From the main Storage Gateway page, you should see the Status as Running. Your screen should look similar to the following.\n5. Create an NFS share on the File Gateway From the main Storage Gateway page, select the DataMigrationGateway and click Create file share.\nClick the Customize configuration button at the bottom of the page.\n For the Amazon S3 bucket name, enter the name of the S3 bucket that DataSync copied the files to. Under Access objects using, select Network File System (NFS). Your screen should look similar to the following. Leave the remaining options to Default  Click Next.\nOn the Amazon S3 Storage configuration page, leave the default settings except under Access to your S3 bucket Leave the defaults, click Next.\nOn the File access settings page, under Access object click Add client.\n Add the IP address to the private IP address of the Application Server followed by a /32 CIDR. Under Mount Options Change the Squash level to No root squash. Leave the remaining options as the defaults.  Click Next.\n6. Review and create Review the configuration of the file share and click Create\nClick on the File share ID that we created and note the mount instructions at the bottom of the page under Example Commands.\nClick on the Copy next to the Linux mount to your favorite text editor.\n7. Mount the NFS Share on the Application Server Return to the SID-appserver instance CLI.\nRun the following command.\nsudo mkdir /mnt/fgw Copy the Linux mount command from the Storage Gateway file share page and replace \u0026ldquo;[MountPath]\u0026rdquo; with \u0026ldquo;/mnt/fgw\u0026rdquo;. You must run the command as sudo\nsudo mount -t nfs -o nolock,hard 10.11.12.133:/sid-datasync-d8b2dd90-f688-11ee-89bf-0e4af0394e11 /mnt/fgw 8. Validate the files exist in both NFS shares Run the following command to verify that the same set of files exists on both NFS shares.\ndiff -qr /mnt/data /mnt/fgw You should see only one extra file in /mnt/fgw: .aws-datasync-metadata. DataSync created this file in the S3 bucket when the task was executed. All other files are the same, showing that DataSync fully transferred our data without errors.\nIn this module you successfully activated the File Gateway and created an NFS file share on the gateway. You then mounted the share on the Application server and verified that the files from the on-premises NFS server were copied correctly to the S3 bucket.\n"
},
{
	"uri": "/2-prerequiste/2.2-deploy-cloudformation-template/",
	"title": "Deploy CloudFormation template ",
	"tags": [],
	"description": "",
	"content": "For the Migrating Data to AWS lab we will need to simulate an on-premise and an in-cloud environment. To do this, we will use two separate regions, us-west-2 (on-premises) and us-east-1 (AWS Cloud). You would deploy the necessary resources for the On-premises environment in us-west-2 and an additional template in us-east-1 for the In-cloud environment in AWS.\nDeploy On-premise simulation Go to Cloudformation service management console. Ensure you are in us-west-2 (Oregon).\n Click Create stack. Enter the following in the Amazon S3 URL field, replacing {AssetsBucketName} with the name of the Amazon S3 bucket you uploaded your templates to. Then click the Next button.  https://s3.amazonaws.com/${AssetsBucketName}/assets/sid-base-template.yaml\nEnter SID as the Stack name.\n Under Parameters you will see Lab modules. Choose true in the Deploy Data Migration Lab. Leave the other lab as false. For the S3 bucket with CloudFormation templates parameter, enter the S3 bucket where you uploaded the templates. For the Prefix to CloudFormation templates parameter, leave the default value of assets.  Click Next.\nAccepts the defaults on the Configure stack options page and click Next.\nScroll to the bottom of the Review pages. In the Capabilties section, select the check boxes at the bottom of the page, acknowledging CloudFormation will create the necessary IAM resources. Click Submit to begin stack creation.\nIt will take between 20-30 minutes for the template to deploy depending on which lab(s) you selected. If you selected all four labs, your screen will look similar to the following.\nWhen deployment complete, check the outputs.\n AppServerPrivateIP – This is the private IP address of the Application Server. You will use this when creating the File Gateway file share to limit access to the NFS export. DataSyncAgentPublicIP – This is the public IP address of the EC2 instance running the DataSync agent. You will use this when activating the DataSync agent. FileGatewayPublicIP – This is the public IP address of the EC2 instance running the File Gateway. You will use this when activating the File Gateway. NfsServerPrivateIP – This is the private IP address of the NFS server. You will use this both on the Application Server and when creating a location for DataSync.  Copy all four Outputs into your favorite text editor. You will need them to complete this lab.\nDeploy AWS Environment Go to Cloudformation service management console. Ensure you are in us-west-2 (Oregon).\n Click Create stack. Enter the following in the Amazon S3 URL field, replacing ${AssetsBucketName} with the name of the Amazon S3 bucket you uploaded your templates to. Then click the Next button.  https://s3.amazonaws.com/${AssetsBucketName}/assets/sid-datamigration-cloud.yaml\nOn the Specify stack details page, enter SID-DS for the Stack name and click Next.\nOn the Configure stack options page, leave the defaults and click Next.\nOn the Review page, confirm your selections and select the check boxes at the bottom of the page, acknowledging CloudFormation will create the necessary IAM resources. Click Submit.\nWhen deployment complete, check the outputs.\n S3BucketName – This is the name of the S3 bucket where the data will be copied to. You will use this when creating a file share on the File Gateway. BucketRoleForDataSync – This is the role that will be used by the DataSync agent to write files to the S3 bucket. You will use this when creating the S3 location for DataSync.  Copy both Outputs into your favorite text editor. You will need them to complete this lab.\n"
},
{
	"uri": "/2-prerequiste/",
	"title": "Preparation ",
	"tags": [],
	"description": "",
	"content": "Before you can start working on the exercises in each lab, you need to deploy the proper resources. We will use AWS CloudFormation to deploy the necessary resources as code.\nComponents:\n On-premises Region: This represents your on-premises environment where data resides. AWS In-cloud Region: This represents the AWS cloud region where your resources are deployed. NFS Server: An NFS (Network File System) server on your on-premises network that stores the data you want to back up or archive. DataSync Agent: A software agent installed on your on-premises server that communicates with the AWS DataSync service. S3 Bucket: An Amazon S3 bucket in your AWS account where the data from your NFS server will be stored. File Gateway (Optional): An AWS Storage Gateway virtual appliance deployed in your on-premises environment that caches data locally before uploading it to S3. It can improve performance and optimize data transfer costs.  Content  2.1 Create S3 2.2 Deploy CloudFormation template 2.3 Edit Security Group  "
},
{
	"uri": "/3-data-migration/",
	"title": "Data migration",
	"tags": [],
	"description": "",
	"content": "In this step you will simulate the migration of a legacy on-premises file system to S3. You will use DataSync to migrate data from on-premises to AWS. You will then use Storage Gateway to present a file system back to on-premises.\n"
},
{
	"uri": "/2-prerequiste/2.3-edit-sg/",
	"title": "Edit Security group ",
	"tags": [],
	"description": "",
	"content": "Change Security Group Inbound Rules Open the Amazon EC2 console. In the navigation pane on the left, choose Security Groups.\nSelect the security group with the Name SID-ssh-sg, click Edit inbound rules.\nAdd a rule to allow HTTP access from your local workstation.\n Click Add rule and slect HTTP from the Type drop down. Under the Source dropdown, select My IP. This will populate the field with the public IP of your your local workstation. Click the Save rules button to apply your chagnes.  You are now ready to begin the labs.\n"
},
{
	"uri": "/3-data-migration/3.3-cutover/",
	"title": "Incremental Copy Cutover",
	"tags": [],
	"description": "",
	"content": "In this exercise, you will perform an incremental data transfer using DataSync. This will get any new files that may have been created after the initial data copy. Once you have verified all files from the on-premises NFS server have been copied, you can proceed to cutover.\n1. Create a new file on the NFS server Connect to SID-appserver via EC2 Instance Connect\nIn the CLI, run the following command.\nsudo cp /mnt/data/images/00001.jpg /mnt/data/new-image.jpg 2. Copy the new file to the S3 bucket You have already created a DataSync task to copy files from the NFS server to the S3 bucket. To copy the new file, you will just re-run the task. DataSync will only copy files that have changed between the source and the destination.\nMake sure you are in the In-cloud region (us-east-1) environment.\nFrom the AWS console, click Services and select DataSync.\nSelect the task you created previously and click the Start button then Start with defaults.\nClick on the Task history tab to monitor the progress.\nClick on the newest task to watch the task progress. It will take a few minutes for the task to complete.\n3. Validate the data transfer to S3 Remaining in the In-cloud region (us-east-1), click services and select S3.\nSelect the sid-datasync-xxxxxxx bucket.\nYou should now see the new-image.jpg you created earlier.\n\r4. Validate the data on the File Gateway Return to the On-premises (us-west-2) environment.\nConnect to SID-appserver via EC2 Instance Connect\nIn the CLI, run the following command.\nls /mnt/data You can see the image exists locally. Now run the following command to see if it exists on the File Gateway you created.\n\rIn the CLI, run the following command.\nls /mnt/fgw Notice the image isn\u0026rsquo;t there. You copied the file from the NFS server to S3 using DataSync and the File Gateway is connected to the S3 bucket. So what\u0026rsquo;s going on? Why can\u0026rsquo;t you see the file on the File Gateway share on the Application server?\n\rIn this case, the file was written to the S3 bucket via DataSync, not through the File Gateway share itself. File Gateway is not aware that there are new objects in the bucket. In order to see the new file on the Application server, you need to refresh the metadata cache on the File Gateway.\nLet\u0026rsquo;s fix this.\nReturn to the In-cloud region (us-east-1) environment.\nFrom the AWS console, click Services and select Storage Gateway.\nClick on File shares\nClick on the checkbox for the file share you created earlier. On the Actions drop-down select Refresh cache and click Start.\nReturn to the On-premises (us-west-2) environment.\nIn your SSH session, run the following command.\nls /mnt/fgw Now you will see the new-image.jpg we created earlier.\n\rIn this module you added a new file to the NFS server prior to cutover. You then ran the DataSync task a second time to pick up any file changes and copy them to S3. Finally, you used the Refresh Cache method to update the metadata on the File Gateway to see the new files in S3.\n"
},
{
	"uri": "/4-cleanup/",
	"title": "Clean Up Resources",
	"tags": [],
	"description": "",
	"content": "We will take the following steps to delete the resources we created in this exercise.\n1. Delete EC2 Instance Go to EC2 service management console in us-west-2.\n Click Instances. Select all SID-xxx instances. Click Instance state. Click Terminate instance, then click Terminate to confirm.   2. Delete Storage Gateway Go to Storage Gateway service management console in us-east-1\n Click Gateways. Click to select DataMigrationGateway. Click Action \u0026gt; Delete gateway , type Delete to delete the gateway.   3. Delete DataSync resources Go to DataSync service management console in us-east-1\nDelete DataSync Tasks.\n Click Tasks. Click to select task-xxx. Click Action \u0026gt; Delete, type Delete to delete the task.  Delete DataSync Locations\n Click Locations. Click to select loc-xxx. Click Action \u0026gt; Delete, type Delete to delete the locations.  Delete DataSync Agents\n Click Agetns. Click to select agent-xxx. Click Delete, type the agent name to delete the agent.   4. Delete S3 Bucket Go to S3 service management console\n Click on the S3 bucket we created for this lab. Click Empty. Enter permanently delete, then click Empty to proceed to delete the object in the bucket. Click Exit.  After deleting all objects in the bucket, click Delete\n 5. Delete VPC Go to VPC service management console\n Click Your VPCs. Click on Lab VPC. Click Actions. Click Delete VPC.  In the confirm box, enter delete to confirm, click Delete to delete Lab VPC and related resources.\n 6. Delete IAM Role Go to IAM service management console\n Click Roles. In the search box, enter SID. Click to select all SID-xxx. Click Delete, click Delete to delete the roles.  "
},
{
	"uri": "/3-data-migration/3.4-optimize/",
	"title": "Optimize Copy Operations",
	"tags": [],
	"description": "",
	"content": "With all the data in the S3 bucket, you are now ready to shut down your NFS server and move exclusively to using the File Gateway. In this module, you will unmount the NFS server and clean up your DataSync resources. You will then write some test files through File Gateway, verifying they end up in the S3 bucket.\nCutover to File Gateway Go to the On-premises (us-west-2) environment if you are not already there.\nConnect to SID-appserver via EC2 Instance Connect\nIn the CLI, run the following command.\nsudo umount /mnt/data Run the following command to create another new file in the S3 bucket through File Gateway.\nsudo cp /mnt/fgw/images/00002.jpg /mnt/fgw/new-image2.jpg Return to the In-cloud region (us-east-1) environment.\nFrom the AWS console, click Services and select S3.\nClick on the sid-datasync-xxxxxx bucket.\nInside the bucket you shold now see the new-images2.jpg.\n\rYour Application server has completed cutover! You can now read all the files that used to be on the NFS server using the File Gateway share. And any new files written to the share will automatically be uploaded to the S3 bucket. You can now shutdown and decommission your NFS server!\nOne benefit of using File Gateway is that it stores files as complete, wholly accessible objects in S3. With your data in S3, you can now use services such as Amazon Athena, Amazon SageMaker, Amazon EMR, and many other AWS services to gain even greater value and insight from your data.\n"
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]